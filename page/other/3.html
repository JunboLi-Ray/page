分词（类似编程的词法分析）

汉语熵大，汉语是否有词

开源：jieba（python）、。。。

算法：
全遍历词典：游标移动，遍历字典，O(n的2次方)
Hash表+滑动窗口：最长，hash找，没有就继续减少一位，继续hash找
trie+自动机：DoubleArrayTrie（空间空洞少）

歧义消解：
问：ab有时候是词有时候是a和b怎么判定
问：如果已经匹配词了，为什么不继续
基于规则或统计
规则：最大向前，分出词条数最少，词性，个性规则（写歧义怎么拆）
统计（主流）：概率最大（词出现次数相乘，取最大，解决“美的”问题，特高词，降低）

专名识别：
后处理程序
特定词组
数字组合
人名处理
内部规律：1+2，1+1，常用姓氏
外部规律：前缀+后导（ceo***），前导+后缀（**老师）

词典组织：
挖掘、去伪词、测试效果、定版

评测体系：
要有对应的场景，准确率不同
